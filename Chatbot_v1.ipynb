{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "1TmO_-FE4ivf"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyush01Bhatt/Deep-Learning/blob/master/Chatbot_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWPiI2DEAmt-",
        "colab_type": "code",
        "outputId": "9e9507c1-ad09-4d57-a7b2-92e74e261e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip -O /tmp/movie_dialogs.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-24 11:47:46--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘/tmp/movie_dialogs.zip’\n",
            "\n",
            "/tmp/movie_dialogs. 100%[===================>]   9.46M  3.46MB/s    in 2.7s    \n",
            "\n",
            "2019-08-24 11:47:49 (3.46 MB/s) - ‘/tmp/movie_dialogs.zip’ saved [9916637/9916637]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceRZ9QTSB4C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/tmp/movie_dialogs.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/tmp/movie_dialogs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ijPQxXcCWWU",
        "colab_type": "code",
        "outputId": "ff9e273b-dbe6-4a81-ca2e-a67076196d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pip install -q tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 118kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 39.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 48.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZy3w0pDDspd",
        "colab_type": "code",
        "outputId": "9af2d336-ba66-46ac-c295-d2006f2d3160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7u6sGFuEHMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open('/tmp/movie_dialogs/cornell movie-dialogs corpus/movie_lines.txt',\n",
        "            encoding = 'utf-8', errors='ignore').read().split('\\n')\n",
        "conversations = open('/tmp/movie_dialogs/cornell movie-dialogs corpus/movie_conversations.txt',\n",
        "            encoding = 'utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1MvTjFhcUk1",
        "colab_type": "code",
        "outputId": "fb718d7e-d080-4bc4-808c-1d53db2386a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(lines[0])\n",
        "print(conversations[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNBdhijRcog_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "  _line = line.split(' +++$+++ ')\n",
        "  if len(_line) == 5:\n",
        "    id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJP8QOxedPRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "  _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \",\"\")\n",
        "  conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7UpXBWsfEDg",
        "colab_type": "code",
        "outputId": "8117cadf-4dcb-4eb7-f28e-3fdb7a981ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(id2line['L1045'])\n",
        "print(conversations_ids[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "They do not!\n",
            "['L194', 'L195', 'L196', 'L197']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I5EA19KfuJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for ids in conversations_ids:\n",
        "  for i in range(len(ids)-1):\n",
        "    questions.append(id2line[ids[i]])\n",
        "    answers.append(id2line[ids[i+1]])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0T0ouuWiN-z",
        "colab_type": "code",
        "outputId": "ed3634d8-a9d3-4c14-cbc1-3bb216c9b74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(f'questions: \\n {questions[:5]}')\n",
        "print(f'answers: \\n {answers[:5]}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "questions: \n",
            " ['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\"]\n",
            "answers: \n",
            " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwsRDy8VowI4",
        "colab_type": "text"
      },
      "source": [
        "#**EDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3rE7y2Mpukq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNSwxyBi94N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lengths_of_questions = [len(text) for text in questions]\n",
        "lengths_of_answers = [len(text) for text in answers]\n",
        "polarity_questions = [TextBlob(text).sentiment.polarity for text in questions]\n",
        "polarity_answers = [TextBlob(text).sentiment.polarity for text in answers]\n",
        "wordcount_questions = [len(text.split(\" \")) for text in questions]\n",
        "wordcount_answers = [len(text.split(\" \")) for text in answers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90T1vEQr17db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordcount_questions = np.array(wordcount_questions)\n",
        "wordcount_answers = np.array(wordcount_answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ2WARdB2Q6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50b830f4-52b0-409b-cb79-aecd8aaa8985"
      },
      "source": [
        "print(f'Number of sentences with len>250 {len(wordcount_questions[wordcount_questions>250])}')\n",
        "print(f'Number of sentences with len<250 {len(wordcount_questions[wordcount_questions<250])}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences with len>250 8\n",
            "Number of sentences with len<250 221608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXHLCA-r22r0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ebe6279-f4ca-4125-c5bc-8dbed2f80a61"
      },
      "source": [
        "print(f'Number of answers with len>250 {len(wordcount_answers[wordcount_answers>250])}')\n",
        "print(f'Number of answers with len<250 {len(wordcount_answers[wordcount_answers<250])}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of answers with len>250 18\n",
            "Number of answers with len<250 221598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4GdOH7_3JyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d77a951f-0104-4fd3-ac3d-476ba6b05596"
      },
      "source": [
        "print(f'max len of questions = {max(wordcount_questions)}')\n",
        "print(f'max len of answers = {max(wordcount_answers)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max len of questions = 1121\n",
            "max len of answers = 1121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1eTB2arrJdl",
        "colab_type": "code",
        "outputId": "5e8a77e8-cde9-4cb7-d671-1b1221d6ddf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sns.distplot(polarity_questions, bins=50, kde=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4605086e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGBNJREFUeJzt3X+w3XWd3/Hna5OCujtKkJTFBDax\nplqkreIdSOvMrgsuBOoYOmVt1C3RTU1dcWvb7SisnaHjj6lud0plqmypZAlWCTS7DmkXmkbAcTpj\nIhd/gMAiV1BICiQSwG6ZRdF3/zifuF+Te3O/uefeey7J8zFz5n6/78/n+/1+7veenNf5/jgnqSok\nSerjF0Y9AEnSC4ehIUnqzdCQJPVmaEiSejM0JEm9GRqSpN4MDUlSb4aGJKk3Q0OS1NviUQ9gtp10\n0km1YsWKUQ9Dkl5Q7rrrrh9U1dLp+k0bGkk2AW8B9lbVGQe1/R7wh8DSqvpBkgCfAi4EngXeVVVf\nb33XA/+mLfqxqtrc6m8ArgNeDNwCfKCqKsmJwI3ACuB7wNuq6qnpxrtixQrGx8en6yZJ6kjy/T79\n+pyeug5YM8kGTgXOAx7plC8AVrXHRuDq1vdE4ArgbOAs4IokS9oyVwPv6Sx3YFuXAbdV1SrgtjYv\nSRqhaUOjqr4C7J+k6Urgg0D3Gw/XAtfXwE7ghCSnAOcDO6pqfzta2AGsaW0vraqdNfjmxOuBizrr\n2tymN3fqkqQRmdGF8CRrgT1V9a2DmpYBj3bmd7fa4eq7J6kDnFxVj7Xpx4GTZzJWSdLsOeIL4Ule\nAvw+g1NT86Jd45jyO9yTbGRwOozTTjttvoYlScecmRxp/A1gJfCtJN8DlgNfT/LLwB7g1E7f5a12\nuPrySeoAT7TTV7Sfe6caUFVdU1VjVTW2dOm0F/8lSTN0xKFRVfdU1V+vqhVVtYLBKaUzq+pxYBtw\nSQZWA8+0U0zbgfOSLGkXwM8Dtre2HyZZ3e68ugS4uW1qG7C+Ta/v1CVJIzJtaCS5Afgq8Ooku5Ns\nOEz3W4CHgAngvwDvA6iq/cBHgTvb4yOtRuvz2bbMd4FbW/0TwG8keRB4c5uXJI1Qjrb/7nVsbKz8\nnIYkHZkkd1XV2HT9/BoRSVJvR93XiEgL1Rd2PTJp/R1ne8efXjg80pAk9WZoSJJ6MzQkSb0ZGpKk\n3gwNSVJvhoYkqTdDQ5LUm6EhSerN0JAk9WZoSJJ6MzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LUm6Eh\nSerN0JAk9WZoSJJ6MzQkSb0ZGpKk3qYNjSSbkuxN8u1O7d8n+fMkdyf5YpITOm2XJ5lI8kCS8zv1\nNa02keSyTn1lkl2tfmOS41r9+DY/0dpXzNYvLUmamT5HGtcBaw6q7QDOqKq/A3wHuBwgyenAOuC1\nbZnPJFmUZBHwaeAC4HTg7a0vwCeBK6vqVcBTwIZW3wA81epXtn6SpBGaNjSq6ivA/oNq/6uqnm+z\nO4HlbXotsKWqnquqh4EJ4Kz2mKiqh6rqR8AWYG2SAOcAW9vym4GLOuva3Ka3Aue2/pKkEZmNaxq/\nDdzappcBj3badrfaVPWXA093AuhA/efW1dqfaf0PkWRjkvEk4/v27Rv6F5IkTW6o0EjyYeB54POz\nM5yZqaprqmqsqsaWLl06yqFI0lFt8UwXTPIu4C3AuVVVrbwHOLXTbXmrMUX9SeCEJIvb0US3/4F1\n7U6yGHhZ6y9JGpEZHWkkWQN8EHhrVT3badoGrGt3Pq0EVgFfA+4EVrU7pY5jcLF8WwubO4CL2/Lr\ngZs761rfpi8Gbu+EkyRpBKY90khyA/Am4KQku4ErGNwtdTywo12b3llV762qe5PcBNzH4LTVpVX1\nk7ae9wPbgUXApqq6t23iQ8CWJB8DvgFc2+rXAp9LMsHgQvy6Wfh9JUlDyNH25n1sbKzGx8dHPQzp\nEF/Y9cik9Xecfdo8j0Q6VJK7qmpsun5+IlyS1JuhIUnqzdCQJPVmaEiSejM0JEm9GRqSpN4MDUlS\nb4aGJKk3Q0OS1JuhIUnqzdCQJPVmaEiSejM0JEm9GRqSpN4MDUlSb4aGJKk3Q0OS1JuhIUnqzdCQ\nJPVmaEiSejM0JEm9TRsaSTYl2Zvk253aiUl2JHmw/VzS6klyVZKJJHcnObOzzPrW/8Ek6zv1NyS5\npy1zVZIcbhuSpNHpc6RxHbDmoNplwG1VtQq4rc0DXACsao+NwNUwCADgCuBs4Czgik4IXA28p7Pc\nmmm2IUkakWlDo6q+Auw/qLwW2NymNwMXderX18BO4IQkpwDnAzuqan9VPQXsANa0tpdW1c6qKuD6\ng9Y12TYkSSMy02saJ1fVY236ceDkNr0MeLTTb3erHa6+e5L64bYhSRqRoS+EtyOEmoWxzHgbSTYm\nGU8yvm/fvrkciiQd02YaGk+0U0u0n3tbfQ9waqff8lY7XH35JPXDbeMQVXVNVY1V1djSpUtn+CtJ\nkqYz09DYBhy4A2o9cHOnfkm7i2o18Ew7xbQdOC/JknYB/Dxge2v7YZLV7a6pSw5a12TbkCSNyOLp\nOiS5AXgTcFKS3QzugvoEcFOSDcD3gbe17rcAFwITwLPAuwGqan+SjwJ3tn4fqaoDF9ffx+AOrRcD\nt7YHh9mGJGlEpg2Nqnr7FE3nTtK3gEunWM8mYNMk9XHgjEnqT062DUnS6PiJcElSb4aGJKk3Q0OS\n1JuhIUnqzdCQJPVmaEiSejM0JEm9GRqSpN4MDUlSb4aGJKk3Q0OS1JuhIUnqzdCQJPVmaEiSejM0\nJEm9GRqSpN4MDUlSb4aGJKk3Q0OS1JuhIUnqzdCQJPU2VGgk+ZdJ7k3y7SQ3JHlRkpVJdiWZSHJj\nkuNa3+Pb/ERrX9FZz+Wt/kCS8zv1Na02keSyYcYqSRrejEMjyTLgnwNjVXUGsAhYB3wSuLKqXgU8\nBWxoi2wAnmr1K1s/kpzelnstsAb4TJJFSRYBnwYuAE4H3t76SpJGZNjTU4uBFydZDLwEeAw4B9ja\n2jcDF7XptW2e1n5ukrT6lqp6rqoeBiaAs9pjoqoeqqofAVtaX0nSiMw4NKpqD/CHwCMMwuIZ4C7g\n6ap6vnXbDSxr08uAR9uyz7f+L+/WD1pmqrokaUSGOT21hME7/5XAK4BfZHB6ad4l2ZhkPMn4vn37\nRjEESTomDHN66s3Aw1W1r6p+DPwp8EbghHa6CmA5sKdN7wFOBWjtLwOe7NYPWmaq+iGq6pqqGquq\nsaVLlw7xK0mSDmeY0HgEWJ3kJe3axLnAfcAdwMWtz3rg5ja9rc3T2m+vqmr1de3uqpXAKuBrwJ3A\nqnY31nEMLpZvG2K8kqQhLZ6+y+SqaleSrcDXgeeBbwDXAH8GbEnysVa7ti1yLfC5JBPAfgYhQFXd\nm+QmBoHzPHBpVf0EIMn7ge0M7szaVFX3znS8kqThZfBm/+gxNjZW4+Pjox6GdIgv7Hpk0vo7zj5t\nnkciHSrJXVU1Nl0/PxEuSerN0JAk9WZoSJJ6MzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LUm6EhSerN\n0JAk9WZoSJJ6MzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LUm6EhSerN0JAk9WZoSJJ6MzQkSb0ZGpKk\n3gwNSVJvQ4VGkhOSbE3y50nuT/L3kpyYZEeSB9vPJa1vklyVZCLJ3UnO7Kxnfev/YJL1nfobktzT\nlrkqSYYZryRpOMMeaXwK+J9V9Rrg7wL3A5cBt1XVKuC2Ng9wAbCqPTYCVwMkORG4AjgbOAu44kDQ\ntD7v6Sy3ZsjxSpKGMOPQSPIy4FeBawGq6kdV9TSwFtjcum0GLmrTa4Hra2AncEKSU4DzgR1Vtb+q\nngJ2AGta20uramdVFXB9Z12SpBEY5khjJbAP+OMk30jy2SS/CJxcVY+1Po8DJ7fpZcCjneV3t9rh\n6rsnqR8iycYk40nG9+3bN8SvJEk6nGFCYzFwJnB1Vb0e+H/81akoANoRQg2xjV6q6pqqGquqsaVL\nl8715iTpmDVMaOwGdlfVrja/lUGIPNFOLdF+7m3te4BTO8svb7XD1ZdPUpckjciMQ6OqHgceTfLq\nVjoXuA/YBhy4A2o9cHOb3gZc0u6iWg08005jbQfOS7KkXQA/D9je2n6YZHW7a+qSzrokSSOweMjl\nfxf4fJLjgIeAdzMIopuSbAC+D7yt9b0FuBCYAJ5tfamq/Uk+CtzZ+n2kqva36fcB1wEvBm5tD0nS\niAwVGlX1TWBskqZzJ+lbwKVTrGcTsGmS+jhwxjBjlCTNHj8RLknqzdCQJPVmaEiSejM0JEm9GRqS\npN4MDUlSb4aGJKk3Q0OS1JuhIUnqzdCQJPVmaEiSejM0JEm9GRqSpN4MDUlSb4aGJKk3Q0OS1Juh\nIUnqzdCQJPVmaEiSejM0JEm9GRqSpN6GDo0ki5J8I8n/aPMrk+xKMpHkxiTHtfrxbX6ita/orOPy\nVn8gyfmd+ppWm0hy2bBjlSQNZzaOND4A3N+Z/yRwZVW9CngK2NDqG4CnWv3K1o8kpwPrgNcCa4DP\ntCBaBHwauAA4HXh76ytJGpGhQiPJcuAfAJ9t8wHOAba2LpuBi9r02jZPaz+39V8LbKmq56rqYWAC\nOKs9Jqrqoar6EbCl9ZUkjciwRxr/Efgg8NM2/3Lg6ap6vs3vBpa16WXAowCt/ZnW/2f1g5aZqi5J\nGpEZh0aStwB7q+quWRzPTMeyMcl4kvF9+/aNejiSdNQa5kjjjcBbk3yPwamjc4BPASckWdz6LAf2\ntOk9wKkArf1lwJPd+kHLTFU/RFVdU1VjVTW2dOnSIX4lSdLhzDg0quryqlpeVSsYXMi+vareCdwB\nXNy6rQdubtPb2jyt/faqqlZf1+6uWgmsAr4G3AmsandjHde2sW2m45UkDW/x9F2O2IeALUk+BnwD\nuLbVrwU+l2QC2M8gBKiqe5PcBNwHPA9cWlU/AUjyfmA7sAjYVFX3zsF4JUk9zUpoVNWXgS+36YcY\n3Pl0cJ+/BH5ziuU/Dnx8kvotwC2zMUZJ0vD8RLgkqTdDQ5LUm6EhSerN0JAk9WZoSJJ6MzQkSb0Z\nGpKk3gwNSVJvhoYkqTdDQ5LUm6EhSerN0JAk9WZoSJJ6MzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LU\nm6EhSerN0JAk9WZoSJJ6MzQkSb3NODSSnJrkjiT3Jbk3yQda/cQkO5I82H4uafUkuSrJRJK7k5zZ\nWdf61v/BJOs79Tckuactc1WSDPPLSpKGM8yRxvPA71XV6cBq4NIkpwOXAbdV1SrgtjYPcAGwqj02\nAlfDIGSAK4CzgbOAKw4ETevzns5ya4YYryRpSDMOjap6rKq+3qb/L3A/sAxYC2xu3TYDF7XptcD1\nNbATOCHJKcD5wI6q2l9VTwE7gDWt7aVVtbOqCri+sy5J0gjMyjWNJCuA1wO7gJOr6rHW9Dhwcpte\nBjzaWWx3qx2uvnuS+mTb35hkPMn4vn37hvpdJElTGzo0kvwS8CfAv6iqH3bb2hFCDbuN6VTVNVU1\nVlVjS5cunevNSdIxa6jQSPLXGATG56vqT1v5iXZqifZzb6vvAU7tLL681Q5XXz5JXZI0IsPcPRXg\nWuD+qvoPnaZtwIE7oNYDN3fql7S7qFYDz7TTWNuB85IsaRfAzwO2t7YfJlndtnVJZ12SpBFYPMSy\nbwT+CXBPkm+22u8DnwBuSrIB+D7wttZ2C3AhMAE8C7wboKr2J/kocGfr95Gq2t+m3wdcB7wYuLU9\nJEkjMuPQqKr/DUz1uYlzJ+lfwKVTrGsTsGmS+jhwxkzHKEmaXX4iXJLUm6EhSerN0JAk9WZoSJJ6\nMzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LUm6EhSerN0JAk9WZoSJJ6MzQkSb0N89XokjSrvrDrkUnr\n7zj7tHkeiabikYYkqTePNKRjgO/gNVs80pAk9WZoSJJ68/SUtIDN9WmlqdY/FU9nydDQUetIX3Bf\nSOf9X0hj1dHF0NARGeWL1ai27btx6a8s+NBIsgb4FLAI+GxVfWLEQ9IL3JGGwFxbaOM5nKM5QD16\n62dBh0aSRcCngd8AdgN3JtlWVfeNdmQa1uFefI70H+lCe9Ed5XgW2r6YD77Yz68FHRrAWcBEVT0E\nkGQLsBYwNGbJQvwHdyy+8B2tFuLza66N8uaF+divCz00lgGPduZ3A2fP1cYW4vn6F4oX+vg1v470\n+TKT59dsPSdn63XhaAnQhR4avSTZCGxss3+R5IEZruok4AcHF98504HNnknHNZd6/s7zPq6eHNeR\ncVxH5iTgB7P1ujCbry/vHG6f/UqfTgs9NPYAp3bml7faz6mqa4Brht1YkvGqGht2PbPNcR0Zx3Vk\nHNeRWajjgvkZ20L/RPidwKokK5McB6wDto14TJJ0zFrQRxpV9XyS9wPbGdxyu6mq7h3xsCTpmLWg\nQwOgqm4BbpmnzQ19imuOOK4j47iOjOM6Mgt1XDAPY0tVzfU2JElHiYV+TUOStIAcc6GR5DeT3Jvk\np0mmvMsgyZokDySZSHJZp74yya5Wv7FdoJ+NcZ2YZEeSB9vPJZP0+fUk3+w8/jLJRa3tuiQPd9pe\nN1/jav1+0tn2tk59lPvrdUm+2v7edyf5x522Wd1fUz1fOu3Ht99/ou2PFZ22y1v9gSTnDzOOGYzr\nXyW5r+2f25L8Sqdt0r/pPI3rXUn2dbb/Tztt69vf/cEk6+d5XFd2xvSdJE932uZyf21KsjfJt6do\nT5Kr2rjvTnJmp21291dVHVMP4G8Brwa+DIxN0WcR8F3glcBxwLeA01vbTcC6Nv1HwO/M0rj+ALis\nTV8GfHKa/icC+4GXtPnrgIvnYH/1GhfwF1PUR7a/gL8JrGrTrwAeA06Y7f11uOdLp8/7gD9q0+uA\nG9v06a3/8cDKtp5F8ziuX+88h37nwLgO9zedp3G9C/hPkyx7IvBQ+7mkTS+Zr3Ed1P93GdycM6f7\nq637V4EzgW9P0X4hcCsQYDWwa6721zF3pFFV91fVdB/++9nXl1TVj4AtwNokAc4BtrZ+m4GLZmlo\na9v6+q73YuDWqnp2lrY/lSMd18+Men9V1Xeq6sE2/X+AvcDSWdp+16TPl8OMdytwbts/a4EtVfVc\nVT0MTLT1zcu4quqOznNoJ4PPQs21PvtrKucDO6pqf1U9BewA1oxoXG8HbpilbR9WVX2FwZvEqawF\nrq+BncAJSU5hDvbXMRcaPU329SXLgJcDT1fV8wfVZ8PJVfVYm34cOHma/us49An78XZoemWS4+d5\nXC9KMp5k54FTZiyg/ZXkLAbvHr/bKc/W/prq+TJpn7Y/nmGwf/osO5fj6trA4N3qAZP9TedzXP+o\n/X22JjnwId8Fsb/aabyVwO2d8lztrz6mGvus768Ff8vtTCT5EvDLkzR9uKpunu/xHHC4cXVnqqqS\nTHlbW3sH8bcZfH7lgMsZvHgex+C2uw8BH5nHcf1KVe1J8krg9iT3MHhhnLFZ3l+fA9ZX1U9becb7\n62iU5LeAMeDXOuVD/qZV9d3J1zDr/jtwQ1U9l+SfMThKO2eett3HOmBrVf2kUxvl/po3R2VoVNWb\nh1zFVF9f8iSDw77F7d3ipF9rMpNxJXkiySlV9Vh7kdt7mFW9DfhiVf24s+4D77qfS/LHwL+ez3FV\n1Z7286EkXwZeD/wJI95fSV4K/BmDNww7O+ue8f6aRJ+vuznQZ3eSxcDLGDyfen1VzhyOiyRvZhDE\nv1ZVzx2oT/E3nY0XwWnHVVVPdmY/y+Aa1oFl33TQsl+ehTH1GlfHOuDSbmEO91cfU4191veXp6cm\nN+nXl9TgytIdDK4nAKwHZuvIZVtbX5/1HnIutb1wHriOcBEw6V0WczGuJEsOnN5JchLwRuC+Ue+v\n9rf7IoNzvVsPapvN/dXn6266470YuL3tn23AugzurloJrAK+NsRYjmhcSV4P/GfgrVW1t1Of9G86\nj+M6pTP7VuD+Nr0dOK+NbwlwHj9/xD2n42pjew2Di8pf7dTmcn/1sQ24pN1FtRp4pr0xmv39NdtX\n+Rf6A/iHDM7rPQc8AWxv9VcAt3T6XQh8h8E7hQ936q9k8I96AvhvwPGzNK6XA7cBDwJfAk5s9TEG\n/2PhgX4rGLx7+IWDlr8duIfBi99/BX5pvsYF/P227W+1nxsWwv4Cfgv4MfDNzuN1c7G/Jnu+MDjd\n9dY2/aL2+0+0/fHKzrIfbss9AFwwy8/36cb1pfbv4MD+2Tbd33SexvXvgHvb9u8AXtNZ9rfbfpwA\n3j2f42rz/xb4xEHLzfX+uoHB3X8/ZvD6tQF4L/De1h4G/2Hdd9v2xzrLzur+8hPhkqTePD0lSerN\n0JAk9WZoSJJ6MzQkSb0ZGpKk3gwNSVJvhoYkqTdDQ5LU2/8HBZ7IoP2kOsEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_t1quzBtlf4",
        "colab_type": "code",
        "outputId": "709843ab-da33-40b9-a5f7-25d663792b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "polarity_questions = np.array(polarity_questions)\n",
        "print(f'Total negative polarity questions {len(polarity_questions[polarity_questions<0])}')\n",
        "print(f'Total positive polarity questions {len(polarity_questions[polarity_questions>0])}')\n",
        "print(f'Total neutral polarity questions {len(polarity_questions[polarity_questions==0])}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total negative polarity questions 33702\n",
            "Total positive polarity questions 53447\n",
            "Total neutral polarity questions 134467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngxm6UYIw4SX",
        "colab_type": "code",
        "outputId": "17d61c95-cc5d-48d1-a8cd-a3dfb93e7279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sns.distplot(polarity_answers, bins=50, kde=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4604f599e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4WxvsMVxXtl",
        "colab_type": "code",
        "outputId": "423c796b-e8f6-45bb-fab2-0b9945926361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "polarity_answers = np.array(polarity_answers)\n",
        "print(f'Total negative polarity answers {len(polarity_answers[polarity_answers<0])}')\n",
        "print(f'Total positive polarity answers {len(polarity_answers[polarity_answers>0])}')\n",
        "print(f'Total neutral polarity answers {len(polarity_answers[polarity_answers==0])}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total negative polarity answers 34304\n",
            "Total positive polarity answers 56397\n",
            "Total neutral polarity answers 130915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7TkuJfhxvY7",
        "colab_type": "code",
        "outputId": "33a35734-876e-45e1-9420-baf0ed3714b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sns.distplot(wordcount_questions,kde=False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4604fd6f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCT1ZmhIknozNCRJvf1/NG1xUCAwkMQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vxBerHYylco",
        "colab_type": "code",
        "outputId": "463e64fc-3549-4630-ec9c-92e81e9c0d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "sns.distplot(wordcount_answers,kde=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4604fd6f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF9lJREFUeJzt3X+sX3Wd5/Hna1thXF2kyN2m25Yt\nasdNJTsVbqDGccLKCIVMLG6ICxrpOMRqhKzumoww8weuPxLcXYcdEu0MSocyUZABHRpTp9PtsGMm\n2SIXIfyU6QVB2hTaoQiz4wQtvveP7+fil+u9t6f3e+HbW56P5OR7zvt8zjmfT06TV8+P7/2mqpAk\nqYt/MewOSJLmD0NDktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSps4XD7sBcO/HE\nE2vFihXD7oYkzSt33XXXP1TVyKHaHXWhsWLFCsbGxobdDUmaV5I83qWdt6ckSZ0ZGpKkzgwNSVJn\nhoYkqbNDhkaS5UluT/JgkgeSfKLVT0iyPcmu9rmo1ZPkmiTjSe5Ncmrfvta39ruSrO+rn5bkvrbN\nNUky0zEkScPR5UrjIPCpqloFrAEuTbIKuBzYUVUrgR1tGeBcYGWbNgAboRcAwJXAGcDpwJV9IbAR\n+EjfdmtbfbpjSJKG4JChUVV7q+oHbf4fgYeApcA6YHNrthk4v82vA26onp3A8UmWAOcA26vqQFU9\nA2wH1rZ1x1XVzur9jOANk/Y11TEkSUNwWM80kqwA3g7cASyuqr1t1ZPA4ja/FHiib7PdrTZTffcU\ndWY4xuR+bUgylmRs//79hzMkSdJh6BwaSV4P3Ap8sqqe61/XrhBe1h8bn+kYVXVtVY1W1ejIyCG/\n0ChJmqVO3whP8hp6gfH1qvpWKz+VZElV7W23mPa1+h5ged/my1ptD3DmpPr/afVlU7Sf6Rgvi2/c\n8eMp6x8446SX87CSNG90eXsqwHXAQ1X1R32rtgATb0CtB27rq1/c3qJaAzzbbjFtA85Osqg9AD8b\n2NbWPZdkTTvWxZP2NdUxJElD0OVK453Ah4D7ktzTan8AXAXcnOQS4HHg/W3dVuA8YBz4KfBhgKo6\nkORzwJ2t3Wer6kCb/zhwPfBa4LttYoZjSJKG4JChUVV/B2Sa1WdN0b6AS6fZ1yZg0xT1MeCUKepP\nT3UMSdJw+I1wSVJnhoYkqTNDQ5LUmaEhSerM0JAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnhoYkqTND\nQ5LUmaEhSerM0JAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnXX4jfFOSfUnu76t9M8k9bXps4mdgk6xI\n8s996/6kb5vTktyXZDzJNe33wElyQpLtSXa1z0WtntZuPMm9SU6d++FLkg5HlyuN64G1/YWq+k9V\ntbqqVgO3At/qW/3IxLqq+lhffSPwEWBlmyb2eTmwo6pWAjvaMsC5fW03tO0lSUN0yNCoqu8BB6Za\n164W3g/cONM+kiwBjquqne03xG8Azm+r1wGb2/zmSfUbqmcncHzbjyRpSAZ9pvEu4Kmq2tVXOznJ\n3Un+Nsm7Wm0psLuvze5WA1hcVXvb/JPA4r5tnphmm5dIsiHJWJKx/fv3DzAcSdJMBg2Ni3jpVcZe\n4KSqejvwX4FvJDmu687aVUgdbieq6tqqGq2q0ZGRkcPdXJLU0cLZbphkIfAfgdMmalX1PPB8m78r\nySPArwN7gGV9my9rNYCnkiypqr3t9tO+Vt8DLJ9mG0nSEAxypfHbwA+r6sXbTklGkixo82+i9xD7\n0Xb76bkka9pzkIuB29pmW4D1bX79pPrF7S2qNcCzfbexJElD0OWV2xuB/wu8NcnuJJe0VRfyqw/A\nfwu4t72CewvwsaqaeIj+ceBrwDjwCPDdVr8KeE+SXfSC6KpW3wo82tp/tW0vSRqiQ96eqqqLpqn/\n7hS1W+m9gjtV+zHglCnqTwNnTVEv4NJD9U+S9MrxG+GSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQ\nJHVmaEiSOjM0JEmdGRqSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQJHVmaEiSOjM0JEmdGRqSpM66\n/NzrpiT7ktzfV/tMkj1J7mnTeX3rrkgynuThJOf01de22niSy/vqJye5o9W/meSYVj+2LY+39Svm\natCSpNnpcqVxPbB2ivrVVbW6TVsBkqyi99vhb2vbfCXJgiQLgC8D5wKrgItaW4Avtn29BXgGmPgN\n8kuAZ1r96tZOkjREhwyNqvoecKDj/tYBN1XV81X1I2AcOL1N41X1aFX9DLgJWJckwLuBW9r2m4Hz\n+/a1uc3fApzV2kuShmSQZxqXJbm33b5a1GpLgSf62uxutenqbwR+UlUHJ9Vfsq+2/tnW/lck2ZBk\nLMnY/v37BxiSJGkmsw2NjcCbgdXAXuBLc9ajWaiqa6tqtKpGR0ZGhtkVSTqqzSo0quqpqnqhqn4B\nfJXe7SeAPcDyvqbLWm26+tPA8UkWTqq/ZF9t/Rtae0nSkMwqNJIs6Vt8HzDxZtUW4ML25tPJwErg\n+8CdwMr2ptQx9B6Wb6mqAm4HLmjbrwdu69vX+jZ/AfA3rb0kaUgWHqpBkhuBM4ETk+wGrgTOTLIa\nKOAx4KMAVfVAkpuBB4GDwKVV9ULbz2XANmABsKmqHmiH+DRwU5LPA3cD17X6dcCfJxmn9yD+woFH\nK0kaSI62/7yPjo7W2NjYrLb9xh0/nrL+gTNOGqRLknTES3JXVY0eqp3fCJckdWZoSJI6MzQkSZ0Z\nGpKkzgwNSVJnhoYkqTNDQ5LUmaEhSerM0JAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnhoYkqTNDQ5LU\nmaEhSerM0JAkdXbI0EiyKcm+JPf31f5Hkh8muTfJt5Mc3+orkvxzknva9Cd925yW5L4k40muSZJW\nPyHJ9iS72ueiVk9rN96Oc+rcD1+SdDi6XGlcD6ydVNsOnFJV/x74e+CKvnWPVNXqNn2sr74R+Aiw\nsk0T+7wc2FFVK4EdbRng3L62G9r2kqQhOmRoVNX3gAOTan9dVQfb4k5g2Uz7SLIEOK6qdlbvR8lv\nAM5vq9cBm9v85kn1G6pnJ3B8248kaUjm4pnG7wHf7Vs+OcndSf42ybtabSmwu6/N7lYDWFxVe9v8\nk8Divm2emGYbSdIQLBxk4yR/CBwEvt5Ke4GTqurpJKcBf5nkbV33V1WVpGbRjw30bmFx0kknHe7m\nkqSOZn2lkeR3gd8BPthuOVFVz1fV023+LuAR4NeBPbz0FtayVgN4auK2U/vc1+p7gOXTbPMSVXVt\nVY1W1ejIyMhshyRJOoRZhUaStcDvA++tqp/21UeSLGjzb6L3EPvRdvvpuSRr2ltTFwO3tc22AOvb\n/PpJ9YvbW1RrgGf7bmNJkobgkLenktwInAmcmGQ3cCW9t6WOBba3N2d3tjelfgv4bJKfA78APlZV\nEw/RP07vTazX0nsGMvEc5Crg5iSXAI8D72/1rcB5wDjwU+DDgwxUkjS4Q4ZGVV00Rfm6adreCtw6\nzbox4JQp6k8DZ01RL+DSQ/VPkvTK8RvhkqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjoz\nNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQkCR1ZmhIkjozNCRJnRkakqTOOoVGkk1J9iW5\nv692QpLtSXa1z0WtniTXJBlPcm+SU/u2Wd/a70qyvq9+WpL72jbXtN8Rn/YYkqTh6HqlcT2wdlLt\ncmBHVa0EdrRlgHOBlW3aAGyEXgDQ+33xM4DTgSv7QmAj8JG+7dYe4hiSpCHoFBpV9T3gwKTyOmBz\nm98MnN9Xv6F6dgLHJ1kCnANsr6oDVfUMsB1Y29YdV1U72++C3zBpX1MdQ5I0BIM801hcVXvb/JPA\n4ja/FHiir93uVpupvnuK+kzHkCQNwZw8CG9XCDUX+5rNMZJsSDKWZGz//v0vZzck6VVtkNB4qt1a\non3ua/U9wPK+dstabab6sinqMx3jJarq2qoararRkZGRAYYkSZrJIKGxBZh4A2o9cFtf/eL2FtUa\n4Nl2i2kbcHaSRe0B+NnAtrbuuSRr2ltTF0/a11THkCQNwcIujZLcCJwJnJhkN723oK4Cbk5yCfA4\n8P7WfCtwHjAO/BT4MEBVHUjyOeDO1u6zVTXxcP3j9N7Qei3w3TYxwzEkSUPQKTSq6qJpVp01RdsC\nLp1mP5uATVPUx4BTpqg/PdUxJEnD4TfCJUmdGRqSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQJHVm\naEiSOjM0JEmdGRqSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQJHVmaEiSOjM0JEmdzTo0krw1yT19\n03NJPpnkM0n29NXP69vmiiTjSR5Ock5ffW2rjSe5vK9+cpI7Wv2bSY6Z/VAlSYOadWhU1cNVtbqq\nVgOn0fs98G+31VdPrKuqrQBJVgEXAm8D1gJfSbIgyQLgy8C5wCrgotYW4IttX28BngEumW1/JUmD\nm6vbU2cBj1TV4zO0WQfcVFXPV9WPgHHg9DaNV9WjVfUz4CZgXZIA7wZuadtvBs6fo/5KkmZhrkLj\nQuDGvuXLktybZFOSRa22FHiir83uVpuu/kbgJ1V1cFJdkjQkA4dGe87wXuAvWmkj8GZgNbAX+NKg\nx+jQhw1JxpKM7d+//+U+nCS9as3Flca5wA+q6imAqnqqql6oql8AX6V3+wlgD7C8b7tlrTZd/Wng\n+CQLJ9V/RVVdW1WjVTU6MjIyB0OSJE1lLkLjIvpuTSVZ0rfufcD9bX4LcGGSY5OcDKwEvg/cCaxs\nb0odQ+9W15aqKuB24IK2/XrgtjnoryRplhYeusn0krwOeA/w0b7yf0+yGijgsYl1VfVAkpuBB4GD\nwKVV9ULbz2XANmABsKmqHmj7+jRwU5LPA3cD1w3SX0nSYAYKjar6J3oPrPtrH5qh/ReAL0xR3wps\nnaL+KL+8vSVJGjK/ES5J6szQkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6szQ\nkCR1ZmhIkjozNCRJnRkakqTODA1JUmeGhiSpM0NDktSZoSFJ6mzg0EjyWJL7ktyTZKzVTkiyPcmu\n9rmo1ZPkmiTjSe5Ncmrffta39ruSrO+rn9b2P962zaB9liTNzlxdafyHqlpdVaNt+XJgR1WtBHa0\nZYBzgZVt2gBshF7IAFcCZ9D7edcrJ4KmtflI33Zr56jPkqTD9HLdnloHbG7zm4Hz++o3VM9O4Pgk\nS4BzgO1VdaCqngG2A2vbuuOqamdVFXBD374kSa+wuQiNAv46yV1JNrTa4qra2+afBBa3+aXAE33b\n7m61meq7p6hLkoZg4Rzs4zerak+Sfw1sT/LD/pVVVUlqDo4zrRZWGwBOOumkl/NQkvSqNvCVRlXt\naZ/7gG/TeybxVLu1RPvc15rvAZb3bb6s1WaqL5uiPrkP11bVaFWNjoyMDDokSdI0BgqNJK9L8q8m\n5oGzgfuBLcDEG1Drgdva/Bbg4vYW1Rrg2XYbaxtwdpJF7QH42cC2tu65JGvaW1MX9+1LkvQKG/T2\n1GLg2+0t2IXAN6rqr5LcCdyc5BLgceD9rf1W4DxgHPgp8GGAqjqQ5HPAna3dZ6vqQJv/OHA98Frg\nu22SJA3BQKFRVY8CvzFF/WngrCnqBVw6zb42AZumqI8BpwzST0nS3PAb4ZKkzgwNSVJnhoYkqTND\nQ5LUmaEhSerM0JAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnhoYkqTNDQ5LUmaEhSerM0JAkdWZoSJI6\nMzQkSZ0ZGpKkzmYdGkmWJ7k9yYNJHkjyiVb/TJI9Se5p03l921yRZDzJw0nO6auvbbXxJJf31U9O\nckerfzPJMbPtryRpcINcaRwEPlVVq4A1wKVJVrV1V1fV6jZtBWjrLgTeBqwFvpJkQZIFwJeBc4FV\nwEV9+/li29dbgGeASwboryRpQLMOjaraW1U/aPP/CDwELJ1hk3XATVX1fFX9CBgHTm/TeFU9WlU/\nA24C1iUJ8G7glrb9ZuD82fZXkjS4OXmmkWQF8Hbgjla6LMm9STYlWdRqS4En+jbb3WrT1d8I/KSq\nDk6qS5KGZODQSPJ64Fbgk1X1HLAReDOwGtgLfGnQY3Tow4YkY0nG9u/f/3IfTpJetQYKjSSvoRcY\nX6+qbwFU1VNV9UJV/QL4Kr3bTwB7gOV9my9rtenqTwPHJ1k4qf4rquraqhqtqtGRkZFBhiRJmsEg\nb08FuA54qKr+qK++pK/Z+4D72/wW4MIkxyY5GVgJfB+4E1jZ3pQ6ht7D8i1VVcDtwAVt+/XAbbPt\nryRpcAsP3WRa7wQ+BNyX5J5W+wN6bz+tBgp4DPgoQFU9kORm4EF6b15dWlUvACS5DNgGLAA2VdUD\nbX+fBm5K8nngbnohJUkaklmHRlX9HZApVm2dYZsvAF+Yor51qu2q6lF+eXtLkjRkfiNcktSZoSFJ\n6myQZxqvGt+448dT1j9wxkmvcE8kabi80pAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnhoYkqTNDQ5LU\nmaEhSerM0JAkdWZoSJI6MzQkSZ0ZGpKkzgwNSVJnhoYkqbMj/k+jJ1kL/DG9n4L9WlVdNeQuvWi6\nP5kO/tl0SUenI/pKI8kC4MvAucAqer8/vmq4vZKkV68jOjTo/T74eFU9WlU/A24C1g25T5L0qnWk\n355aCjzRt7wbOGNIfTksM926moq3syTNB0d6aHSSZAOwoS3+vyQPz3JXJwL/MDe9OjwffHl3P7Rx\nvQKO1rE5rvlnvo/t33ZpdKSHxh5ged/yslZ7iaq6Frh20IMlGauq0UH3c6Q5WscFR+/YHNf8czSP\nrd+R/kzjTmBlkpOTHANcCGwZcp8k6VXriL7SqKqDSS4DttF75XZTVT0w5G5J0qvWER0aAFW1Fdj6\nCh1u4FtcR6ijdVxw9I7Ncc0/R/PYXpSqGnYfJEnzxJH+TEOSdAQxNJoka5M8nGQ8yeXD7s/hSLI8\nye1JHkzyQJJPtPoJSbYn2dU+F7V6klzTxnpvklOHO4KZJVmQ5O4k32nLJye5o/X/m+0lCZIc25bH\n2/oVw+z3TJIcn+SWJD9M8lCSdxxF5+u/tH+H9ye5McmvzcdzlmRTkn1J7u+rHfY5SrK+td+VZP0w\nxjKXDA2Oij9XchD4VFWtAtYAl7b+Xw7sqKqVwI62DL1xrmzTBmDjK9/lw/IJ4KG+5S8CV1fVW4Bn\ngEta/RLgmVa/urU7Uv0x8FdV9e+A36A3vnl/vpIsBf4zMFpVp9B7geVC5uc5ux5YO6l2WOcoyQnA\nlfS+lHw6cOVE0MxbVfWqn4B3ANv6lq8Arhh2vwYYz23Ae4CHgSWttgR4uM3/KXBRX/sX2x1pE73v\n5uwA3g18Bwi9L1AtnHzu6L1l9442v7C1y7DHMMWY3gD8aHLfjpLzNfFXHE5o5+A7wDnz9ZwBK4D7\nZ3uOgIuAP+2rv6TdfJy80uiZ6s+VLB1SXwbSLu/fDtwBLK6qvW3Vk8DiNj+fxvu/gN8HftGW3wj8\npKoOtuX+vr84rrb+2db+SHMysB/4s3bb7WtJXsdRcL6qag/wP4EfA3vpnYO7mP/nbMLhnqN5c+66\nMjSOIkleD9wKfLKqnutfV73/5syrV+WS/A6wr6ruGnZf5thC4FRgY1W9HfgnfnmbA5if5wug3XpZ\nRy8Y/w3wOn71Fs9RYb6eo0EZGj2d/lzJkSzJa+gFxter6lut/FSSJW39EmBfq8+X8b4TeG+Sx+j9\nheN303sWcHySie8Y9ff9xXG19W8Ann4lO9zRbmB3Vd3Rlm+hFyLz/XwB/Dbwo6raX1U/B75F7zzO\n93M24XDP0Xw6d50YGj3z+s+VJAlwHfBQVf1R36otwMTbGuvpPeuYqF/c3vhYAzzbd8l9xKiqK6pq\nWVWtoHdO/qaqPgjcDlzQmk0e18R4L2jtj7j/CVbVk8ATSd7aSmcBDzLPz1fzY2BNkn/Z/l1OjG1e\nn7M+h3uOtgFnJ1nUrsLObrX5a9gPVY6UCTgP+HvgEeAPh92fw+z7b9K7TL4XuKdN59G7N7wD2AX8\nb+CE1j703hZ7BLiP3psuQx/HIcZ4JvCdNv8m4PvAOPAXwLGt/mttebytf9Ow+z3DeFYDY+2c/SWw\n6Gg5X8B/A34I3A/8OXDsfDxnwI30nsv8nN7V4SWzOUfA77XxjQMfHva4Bp38RrgkqTNvT0mSOjM0\nJEmdGRqSpM4MDUlSZ4aGJKkzQ0OS1JmhIUnqzNCQJHX2/wFqHoGEsN/ongAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "payRGn-n8dG_",
        "colab_type": "text"
      },
      "source": [
        "#**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9wE67pL7_wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "  \n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"i'm\",\"i am\", text)\n",
        "  text = re.sub(r\"he's\",\"he is\", text)\n",
        "  text = re.sub(r\"she's\",\"she is\", text)\n",
        "  text = re.sub(r\"that's\",\"that is\", text)\n",
        "  text = re.sub(r\"what's\",\"what is\", text)\n",
        "  text = re.sub(r\"where's\",\"where is\", text)\n",
        "  text = re.sub(r\"\\'ll\",\" will\", text)\n",
        "  text = re.sub(r\"\\'ve\",\" have\", text)\n",
        "  text = re.sub(r\"\\'d\",\" would\", text)\n",
        "  text = re.sub(r\"\\'re\",\" are\", text)\n",
        "  text = re.sub(r\"won't\",\"will not\", text)\n",
        "  text = re.sub(r\"can't\",\"cannot\", text)\n",
        "  text = re.sub(r\"don't\",\"do not\", text)\n",
        "  text = re.sub(r\"[-+(){}#|@=]+\",\"\", text)\n",
        "  text = re.sub(r\"([?.!,।])\",r\" \\1 \",text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  text = re.sub(r\"[^a-zA-Z?.!,]+\",\" \", text)\n",
        "  text = text.rstrip().strip()\n",
        "  text = '<start> ' + text + ' <end>'\n",
        " \n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWYJW_5A05B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_questions = []\n",
        "for ques in questions:\n",
        "  clean_questions.append(clean_text(ques))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aykLT1vo9CcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_answers = []\n",
        "for ans in answers:\n",
        "  clean_answers.append(clean_text(ans))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55SXJEvl-HIS",
        "colab_type": "code",
        "outputId": "98410670-a226-4ed1-e9c7-da00a74d722a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(clean_questions[0:5])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again . <end>', '<start> well , i thought we would start with pronunciation , if that is okay with you . <end>', '<start> not the hacking and gagging and spitting part . please . <end>', '<start> you are asking me out . that is so cute . what is your name again ? <end>', '<start> no , no , it s my fault we didn t have a proper introduction <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s02JP6kx5TGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordcount_clean_ques = np.array([len(text.split(\" \")) for text in clean_questions])\n",
        "wordcount_clean_ans = np.array([len(text.split(\" \")) for text in clean_answers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhOYSdVy5u4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0844eacb-71d5-4baf-d0a4-a24a7aedd5b5"
      },
      "source": [
        "print(f'max len of questions = {max(wordcount_clean_ques)}')\n",
        "print(f'max len of answers = {max(wordcount_clean_ans)}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max len of questions = 390\n",
            "max len of answers = 740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or7NRvy_6CuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ece9a71f-8fed-40eb-fd63-9363a90718aa"
      },
      "source": [
        "print(f'Number of sentences with len>0 {len(wordcount_clean_ques[(wordcount_clean_ques>10) & (wordcount_clean_ques<50)])}')\n",
        "print(f'Number of sentences with len<50 {len(wordcount_clean_ques[wordcount_clean_ques<20])}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences with len>0 111419\n",
            "Number of sentences with len<50 170813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj1j_qZF6gzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a10b065b-d7e9-417d-a894-2bb968dee7b5"
      },
      "source": [
        "print(f'Number of answers with len>250 {len(wordcount_clean_ans[wordcount_clean_ans>10])}')\n",
        "print(f'Number of answers with len<250 {len(wordcount_clean_ans[wordcount_clean_ans<20])}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of answers with len>250 119898\n",
            "Number of answers with len<250 167669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcbhQ-df61Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_questions = np.array(clean_questions)\n",
        "clean_answers = np.array(clean_answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0NrI0o17BzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = wordcount_clean_ans < 20\n",
        "clean_questions = clean_questions[mask]\n",
        "clean_answers = clean_answers[mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jJgCn4u7jBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42a66068-0d06-4b01-f641-cf31955d4b52"
      },
      "source": [
        "len(clean_questions)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F21-aH599VlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordcount_clean_ques2 = np.array([len(text.split(\" \")) for text in clean_questions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQr5DZGG8qvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask2 = wordcount_clean_ques2 < 20\n",
        "clean_questions = clean_questions[mask2]\n",
        "clean_answers = clean_answers[mask2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5HciTxPg8au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d22440e-2127-4249-cfc8-75597ce0d220"
      },
      "source": [
        "len(clean_questions)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyd9w6U2z0Hk",
        "colab_type": "text"
      },
      "source": [
        "#**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1FlASXY-b74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count = {}\n",
        "\n",
        "for ques in questions:\n",
        "  for word in ques.split(\" \"):\n",
        "    if word in word_count:\n",
        "      word_count[word] += 1\n",
        "    else:\n",
        "      word_count[word] = 1\n",
        "\n",
        "for ans in answers:\n",
        "  for word in ans.split(\" \"):\n",
        "    if word in word_count:\n",
        "      word_count[word] += 1\n",
        "    else:\n",
        "      word_count[word] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8cDMP5qhMnC",
        "colab_type": "code",
        "outputId": "e88c7b5e-62aa-4a15-9fe7-d310d35328f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_count)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l748ZUVWzdax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token = '<oov>')\n",
        "lang_tokenizer.fit_on_texts(np.concatenate((clean_questions, clean_answers), axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9XHeoUW1E2j",
        "colab_type": "code",
        "outputId": "a943f0f5-16e0-455a-cca5-ab9c0f21a867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = lang_tokenizer.word_index\n",
        "len(vocab)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khVQPBx51mb3",
        "colab_type": "code",
        "outputId": "275d8bfd-deeb-431a-bb17-5a4fad55ed12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ques_tensor = lang_tokenizer.texts_to_sequences(clean_questions)\n",
        "ques_tensor = tf.keras.preprocessing.sequence.pad_sequences(ques_tensor,padding='post')\n",
        "print(ques_tensor[0:1])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    3    61     7     8   134    25    51   351    47 21818     7    76\n",
            "     18    12    84    47     6     2     4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yZC9kvV2kIL",
        "colab_type": "code",
        "outputId": "b3e8d48c-57e9-4d29-b111-95f1af4659c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ques_tensor[0])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUWB_DRuIj5S",
        "colab_type": "code",
        "outputId": "dcf7aaa8-894c-428e-9b9a-4471790c0240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ques_tensor_shape = ques_tensor.shape\n",
        "print(ques_tensor_shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(129512, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otMDkWpf2wVj",
        "colab_type": "code",
        "outputId": "b2dcc9b9-43dd-4d59-8dfe-ac7fab3a31f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ans_tensor = lang_tokenizer.texts_to_sequences(clean_answers)\n",
        "ans_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_tensor,padding='post')\n",
        "print(ans_tensor[0:1])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    3    17     9  6906    29 12263    29  8977   473     2   137     2\n",
            "      4     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mfGl65H3Nme",
        "colab_type": "code",
        "outputId": "d15c2d32-6df2-41af-ce9c-41d97bd92134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ans_tensor[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnQx0ypzv8Rg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "098fe745-a43b-49a8-e877-6d1cc180ac92"
      },
      "source": [
        "ans_tensor_shape = ans_tensor.shape\n",
        "print(ans_tensor_shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(129512, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0J143x33S9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rev_word_index = {v:k for k,v in vocab.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDQazY9y32ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(ques_tensor)\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = len(ques_tensor)//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 128\n",
        "vocab_input_size = len(vocab)+1\n",
        "vocab_tar_size = len(vocab)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((ques_tensor,ans_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0ylAK04UBv",
        "colab_type": "code",
        "outputId": "f842e443-ca8a-4d49-a796-199cc305a0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ex_inp_batch, ex_tar_batch = next(iter(dataset))\n",
        "ex_inp_batch.shape, ex_tar_batch.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 19]), TensorShape([32, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TmO_-FE4ivf",
        "colab_type": "text"
      },
      "source": [
        "#**Encoder Decoder with Attention(Bahdanau)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubeVuJ-NIPYJ",
        "colab_type": "text"
      },
      "source": [
        "##**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRSWwLKMIVDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru1 = tf.keras.layers.GRU(self.enc_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    #self.gru2 = tf.keras.layers.GRU(self.enc_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    #self.gru3 = tf.keras.layers.GRU(self.enc_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    self.gru4 = tf.keras.layers.GRU(self.enc_units,return_sequences = True,return_state = True,recurrent_initializer = 'glorot_uniform')\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    x = self.gru1(x, initial_state = hidden)\n",
        "    #x = self.gru2(x)\n",
        "    #x = self.gru3(x)\n",
        "    output,state = self.gru4(x)\n",
        "    return output, state\n",
        "    \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-F1KpJtKpTc",
        "colab_type": "code",
        "outputId": "9792dbc7-fe0a-48b5-c48e-e5265fded055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# SAMPLE INPUT\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(ex_inp_batch, sample_hidden)\n",
        "\n",
        "print (f'Encoder output shape: (batch size, sequence length, units) {sample_output.shape}')\n",
        "print (f'Encoder Hidden state shape: (batch size, units) {sample_hidden.shape}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 19, 128)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8iVx4-JK0vH",
        "colab_type": "text"
      },
      "source": [
        "##**Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtzPxrYXspv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query,1) # dimension == (batch,1,hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)\n",
        "    ))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1) # dimension = (batch, max_seq_length, 1)\n",
        "\n",
        "    context_vector = attention_weights * values  # dimension = (batch, max_seq_length,hidden_size)\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) #dimension = (batch, hidden_size)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqQqGVUQLhlQ",
        "colab_type": "code",
        "outputId": "99a89ada-e2f5-41b1-e6c4-9ced912f7b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (32, 128)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (32, 19, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74eS3vGzL65W",
        "colab_type": "text"
      },
      "source": [
        "##**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jMNfY9PLljP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru1 = tf.keras.layers.GRU(self.dec_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    #self.gru2 = tf.keras.layers.GRU(self.dec_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    #self.gru3 = tf.keras.layers.GRU(self.dec_units,return_sequences = True,recurrent_initializer = 'glorot_uniform')\n",
        "    self.gru4 = tf.keras.layers.GRU(self.dec_units,return_sequences = True,return_state = True,recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x) # dims_out = (batch, 1, embedding_dims)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, axis=1),x], axis=-1)\n",
        "    x = self.gru1(x)\n",
        "    #x = self.gru2(x)\n",
        "    #x = self.gru3(x)\n",
        "    output,state = self.gru4(x)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFIa_rXRZpR",
        "colab_type": "code",
        "outputId": "4f14d295-2303-44a4-d8ab-c390ef5abea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((32, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print (f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 31815)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIGirnxZSFox",
        "colab_type": "text"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV81brRYSjUk",
        "colab_type": "text"
      },
      "source": [
        "##**Optimiser and Loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCAX33YNSAYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK4SytUwULyp",
        "colab_type": "text"
      },
      "source": [
        "##**Creating Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pc6rQV6UKYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1cmbQVrUb5d",
        "colab_type": "text"
      },
      "source": [
        "##**Custom training using Gradient Tape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJax-SQSUToF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp,tar,enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp,enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([vocab['<start>']]*BATCH_SIZE,1)\n",
        "    for t in range(1, tar.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(tar[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(tar[:, t], 1)\n",
        "    \n",
        "    batch_loss = (loss / int(tar.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lv8gT8cUnWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5418a227-728b-44ea-d2d6-3a8ff207c979"
      },
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 200 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.4506\n",
            "Epoch 1 Batch 20 Loss 3.6493\n",
            "Epoch 1 Batch 40 Loss 2.5771\n",
            "Epoch 1 Batch 60 Loss 2.8544\n",
            "Epoch 1 Batch 80 Loss 2.3769\n",
            "Epoch 1 Batch 100 Loss 2.8460\n",
            "Epoch 1 Batch 120 Loss 2.1686\n",
            "Epoch 1 Batch 140 Loss 2.5521\n",
            "Epoch 1 Batch 160 Loss 2.6798\n",
            "Epoch 1 Batch 180 Loss 2.3241\n",
            "Epoch 1 Batch 200 Loss 1.8703\n",
            "Epoch 1 Batch 220 Loss 2.4333\n",
            "Epoch 1 Batch 240 Loss 2.3488\n",
            "Epoch 1 Batch 260 Loss 2.4345\n",
            "Epoch 1 Batch 280 Loss 2.3692\n",
            "Epoch 1 Batch 300 Loss 2.2824\n",
            "Epoch 1 Batch 320 Loss 2.2667\n",
            "Epoch 1 Batch 340 Loss 1.9657\n",
            "Epoch 1 Batch 360 Loss 2.3611\n",
            "Epoch 1 Batch 380 Loss 2.2284\n",
            "Epoch 1 Batch 400 Loss 2.5678\n",
            "Epoch 1 Batch 420 Loss 2.6240\n",
            "Epoch 1 Batch 440 Loss 2.0561\n",
            "Epoch 1 Batch 460 Loss 2.3567\n",
            "Epoch 1 Batch 480 Loss 2.1303\n",
            "Epoch 1 Batch 500 Loss 2.8329\n",
            "Epoch 1 Batch 520 Loss 2.3910\n",
            "Epoch 1 Batch 540 Loss 2.1006\n",
            "Epoch 1 Batch 560 Loss 2.6414\n",
            "Epoch 1 Batch 580 Loss 2.5829\n",
            "Epoch 1 Batch 600 Loss 2.5536\n",
            "Epoch 1 Batch 620 Loss 2.4913\n",
            "Epoch 1 Batch 640 Loss 2.3187\n",
            "Epoch 1 Batch 660 Loss 2.1951\n",
            "Epoch 1 Batch 680 Loss 2.3163\n",
            "Epoch 1 Batch 700 Loss 2.2472\n",
            "Epoch 1 Batch 720 Loss 1.8233\n",
            "Epoch 1 Batch 740 Loss 2.8268\n",
            "Epoch 1 Batch 760 Loss 2.1997\n",
            "Epoch 1 Batch 780 Loss 2.3179\n",
            "Epoch 1 Batch 800 Loss 2.2121\n",
            "Epoch 1 Batch 820 Loss 2.4148\n",
            "Epoch 1 Batch 840 Loss 2.8436\n",
            "Epoch 1 Batch 860 Loss 2.4440\n",
            "Epoch 1 Batch 880 Loss 2.2079\n",
            "Epoch 1 Batch 900 Loss 2.3870\n",
            "Epoch 1 Batch 920 Loss 2.6348\n",
            "Epoch 1 Batch 940 Loss 2.0347\n",
            "Epoch 1 Batch 960 Loss 1.8113\n",
            "Epoch 1 Batch 980 Loss 2.3803\n",
            "Epoch 1 Batch 1000 Loss 2.3511\n",
            "Epoch 1 Batch 1020 Loss 2.4911\n",
            "Epoch 1 Batch 1040 Loss 2.0985\n",
            "Epoch 1 Batch 1060 Loss 2.3722\n",
            "Epoch 1 Batch 1080 Loss 2.0483\n",
            "Epoch 1 Batch 1100 Loss 2.2633\n",
            "Epoch 1 Batch 1120 Loss 2.0586\n",
            "Epoch 1 Batch 1140 Loss 2.2181\n",
            "Epoch 1 Batch 1160 Loss 2.2739\n",
            "Epoch 1 Batch 1180 Loss 1.8065\n",
            "Epoch 1 Batch 1200 Loss 2.3001\n",
            "Epoch 1 Batch 1220 Loss 1.9290\n",
            "Epoch 1 Batch 1240 Loss 2.3659\n",
            "Epoch 1 Batch 1260 Loss 2.2629\n",
            "Epoch 1 Batch 1280 Loss 1.9675\n",
            "Epoch 1 Batch 1300 Loss 2.2160\n",
            "Epoch 1 Batch 1320 Loss 2.2529\n",
            "Epoch 1 Batch 1340 Loss 1.8150\n",
            "Epoch 1 Batch 1360 Loss 2.5024\n",
            "Epoch 1 Batch 1380 Loss 2.3300\n",
            "Epoch 1 Batch 1400 Loss 1.8079\n",
            "Epoch 1 Batch 1420 Loss 2.0614\n",
            "Epoch 1 Batch 1440 Loss 1.7242\n",
            "Epoch 1 Batch 1460 Loss 1.9121\n",
            "Epoch 1 Batch 1480 Loss 2.2045\n",
            "Epoch 1 Batch 1500 Loss 2.0723\n",
            "Epoch 1 Batch 1520 Loss 1.9252\n",
            "Epoch 1 Batch 1540 Loss 2.0350\n",
            "Epoch 1 Batch 1560 Loss 1.9145\n",
            "Epoch 1 Batch 1580 Loss 1.6100\n",
            "Epoch 1 Batch 1600 Loss 1.8785\n",
            "Epoch 1 Batch 1620 Loss 1.9268\n",
            "Epoch 1 Batch 1640 Loss 2.0148\n",
            "Epoch 1 Batch 1660 Loss 2.2823\n",
            "Epoch 1 Batch 1680 Loss 1.9190\n",
            "Epoch 1 Batch 1700 Loss 1.7646\n",
            "Epoch 1 Batch 1720 Loss 2.1575\n",
            "Epoch 1 Batch 1740 Loss 2.0005\n",
            "Epoch 1 Batch 1760 Loss 2.0699\n",
            "Epoch 1 Batch 1780 Loss 2.3142\n",
            "Epoch 1 Batch 1800 Loss 2.0898\n",
            "Epoch 1 Batch 1820 Loss 1.9348\n",
            "Epoch 1 Batch 1840 Loss 1.8577\n",
            "Epoch 1 Batch 1860 Loss 1.8981\n",
            "Epoch 1 Batch 1880 Loss 2.1799\n",
            "Epoch 1 Batch 1900 Loss 2.3504\n",
            "Epoch 1 Batch 1920 Loss 2.0090\n",
            "Epoch 1 Batch 1940 Loss 2.3328\n",
            "Epoch 1 Batch 1960 Loss 1.9370\n",
            "Epoch 1 Batch 1980 Loss 1.9311\n",
            "Epoch 1 Batch 2000 Loss 1.7385\n",
            "Epoch 1 Batch 2020 Loss 1.9801\n",
            "Epoch 1 Batch 2040 Loss 1.9972\n",
            "Epoch 1 Batch 2060 Loss 2.4366\n",
            "Epoch 1 Batch 2080 Loss 1.8336\n",
            "Epoch 1 Batch 2100 Loss 2.1847\n",
            "Epoch 1 Batch 2120 Loss 1.9198\n",
            "Epoch 1 Batch 2140 Loss 2.2110\n",
            "Epoch 1 Batch 2160 Loss 2.0548\n",
            "Epoch 1 Batch 2180 Loss 2.0174\n",
            "Epoch 1 Batch 2200 Loss 1.7600\n",
            "Epoch 1 Batch 2220 Loss 2.0526\n",
            "Epoch 1 Batch 2240 Loss 2.1423\n",
            "Epoch 1 Batch 2260 Loss 2.1543\n",
            "Epoch 1 Batch 2280 Loss 2.0264\n",
            "Epoch 1 Batch 2300 Loss 2.2878\n",
            "Epoch 1 Batch 2320 Loss 1.7478\n",
            "Epoch 1 Batch 2340 Loss 1.8298\n",
            "Epoch 1 Batch 2360 Loss 2.2492\n",
            "Epoch 1 Batch 2380 Loss 1.8827\n",
            "Epoch 1 Batch 2400 Loss 2.1236\n",
            "Epoch 1 Batch 2420 Loss 2.0372\n",
            "Epoch 1 Batch 2440 Loss 2.1543\n",
            "Epoch 1 Batch 2460 Loss 2.1930\n",
            "Epoch 1 Batch 2480 Loss 1.8496\n",
            "Epoch 1 Batch 2500 Loss 1.8698\n",
            "Epoch 1 Batch 2520 Loss 2.0389\n",
            "Epoch 1 Batch 2540 Loss 1.9303\n",
            "Epoch 1 Batch 2560 Loss 2.0482\n",
            "Epoch 1 Batch 2580 Loss 1.8004\n",
            "Epoch 1 Batch 2600 Loss 2.3538\n",
            "Epoch 1 Batch 2620 Loss 2.3322\n",
            "Epoch 1 Batch 2640 Loss 1.7708\n",
            "Epoch 1 Batch 2660 Loss 1.8178\n",
            "Epoch 1 Batch 2680 Loss 1.8235\n",
            "Epoch 1 Batch 2700 Loss 2.2303\n",
            "Epoch 1 Batch 2720 Loss 1.8698\n",
            "Epoch 1 Batch 2740 Loss 1.5448\n",
            "Epoch 1 Batch 2760 Loss 1.7703\n",
            "Epoch 1 Batch 2780 Loss 1.9792\n",
            "Epoch 1 Batch 2800 Loss 1.7625\n",
            "Epoch 1 Batch 2820 Loss 1.8586\n",
            "Epoch 1 Batch 2840 Loss 2.0816\n",
            "Epoch 1 Batch 2860 Loss 2.1135\n",
            "Epoch 1 Batch 2880 Loss 1.7606\n",
            "Epoch 1 Batch 2900 Loss 1.8939\n",
            "Epoch 1 Batch 2920 Loss 1.7945\n",
            "Epoch 1 Batch 2940 Loss 1.8611\n",
            "Epoch 1 Batch 2960 Loss 2.0799\n",
            "Epoch 1 Batch 2980 Loss 2.1017\n",
            "Epoch 1 Batch 3000 Loss 2.3666\n",
            "Epoch 1 Batch 3020 Loss 1.8489\n",
            "Epoch 1 Batch 3040 Loss 2.2776\n",
            "Epoch 1 Batch 3060 Loss 2.2312\n",
            "Epoch 1 Batch 3080 Loss 2.0295\n",
            "Epoch 1 Batch 3100 Loss 2.1995\n",
            "Epoch 1 Batch 3120 Loss 1.8558\n",
            "Epoch 1 Batch 3140 Loss 2.0816\n",
            "Epoch 1 Batch 3160 Loss 1.9868\n",
            "Epoch 1 Batch 3180 Loss 1.9018\n",
            "Epoch 1 Batch 3200 Loss 1.6361\n",
            "Epoch 1 Batch 3220 Loss 1.3850\n",
            "Epoch 1 Batch 3240 Loss 1.9441\n",
            "Epoch 1 Batch 3260 Loss 2.2484\n",
            "Epoch 1 Batch 3280 Loss 2.1072\n",
            "Epoch 1 Batch 3300 Loss 1.7277\n",
            "Epoch 1 Batch 3320 Loss 1.6862\n",
            "Epoch 1 Batch 3340 Loss 1.8628\n",
            "Epoch 1 Batch 3360 Loss 1.9879\n",
            "Epoch 1 Batch 3380 Loss 1.8566\n",
            "Epoch 1 Batch 3400 Loss 2.2556\n",
            "Epoch 1 Batch 3420 Loss 1.9342\n",
            "Epoch 1 Batch 3440 Loss 1.8463\n",
            "Epoch 1 Batch 3460 Loss 1.6498\n",
            "Epoch 1 Batch 3480 Loss 1.7426\n",
            "Epoch 1 Batch 3500 Loss 2.1529\n",
            "Epoch 1 Batch 3520 Loss 1.8945\n",
            "Epoch 1 Batch 3540 Loss 1.9316\n",
            "Epoch 1 Batch 3560 Loss 2.2140\n",
            "Epoch 1 Batch 3580 Loss 2.0496\n",
            "Epoch 1 Batch 3600 Loss 2.0765\n",
            "Epoch 1 Batch 3620 Loss 1.9002\n",
            "Epoch 1 Batch 3640 Loss 2.0185\n",
            "Epoch 1 Batch 3660 Loss 1.7369\n",
            "Epoch 1 Batch 3680 Loss 1.7877\n",
            "Epoch 1 Batch 3700 Loss 2.2497\n",
            "Epoch 1 Batch 3720 Loss 2.1445\n",
            "Epoch 1 Batch 3740 Loss 2.0449\n",
            "Epoch 1 Batch 3760 Loss 1.9598\n",
            "Epoch 1 Batch 3780 Loss 2.1856\n",
            "Epoch 1 Batch 3800 Loss 1.8125\n",
            "Epoch 1 Batch 3820 Loss 1.6092\n",
            "Epoch 1 Batch 3840 Loss 2.0564\n",
            "Epoch 1 Batch 3860 Loss 1.8930\n",
            "Epoch 1 Batch 3880 Loss 2.0807\n",
            "Epoch 1 Batch 3900 Loss 1.7006\n",
            "Epoch 1 Batch 3920 Loss 1.8179\n",
            "Epoch 1 Batch 3940 Loss 2.2511\n",
            "Epoch 1 Batch 3960 Loss 1.9946\n",
            "Epoch 1 Batch 3980 Loss 2.1182\n",
            "Epoch 1 Batch 4000 Loss 1.7316\n",
            "Epoch 1 Batch 4020 Loss 1.8082\n",
            "Epoch 1 Batch 4040 Loss 1.9558\n",
            "Epoch 1 Loss 2.1462\n",
            "Time taken for 1 epoch 969.0967381000519 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8800\n",
            "Epoch 2 Batch 20 Loss 1.8101\n",
            "Epoch 2 Batch 40 Loss 1.8700\n",
            "Epoch 2 Batch 60 Loss 2.1322\n",
            "Epoch 2 Batch 80 Loss 1.7926\n",
            "Epoch 2 Batch 100 Loss 2.1398\n",
            "Epoch 2 Batch 120 Loss 1.5949\n",
            "Epoch 2 Batch 140 Loss 1.8993\n",
            "Epoch 2 Batch 160 Loss 2.0443\n",
            "Epoch 2 Batch 180 Loss 1.7281\n",
            "Epoch 2 Batch 200 Loss 1.3253\n",
            "Epoch 2 Batch 220 Loss 1.8408\n",
            "Epoch 2 Batch 240 Loss 1.7706\n",
            "Epoch 2 Batch 260 Loss 1.8185\n",
            "Epoch 2 Batch 280 Loss 1.7862\n",
            "Epoch 2 Batch 300 Loss 1.7648\n",
            "Epoch 2 Batch 320 Loss 1.7540\n",
            "Epoch 2 Batch 340 Loss 1.5199\n",
            "Epoch 2 Batch 360 Loss 1.9164\n",
            "Epoch 2 Batch 380 Loss 1.7952\n",
            "Epoch 2 Batch 400 Loss 2.1920\n",
            "Epoch 2 Batch 420 Loss 2.1528\n",
            "Epoch 2 Batch 440 Loss 1.6461\n",
            "Epoch 2 Batch 460 Loss 1.9014\n",
            "Epoch 2 Batch 480 Loss 1.7242\n",
            "Epoch 2 Batch 500 Loss 2.3832\n",
            "Epoch 2 Batch 520 Loss 2.0001\n",
            "Epoch 2 Batch 540 Loss 1.7417\n",
            "Epoch 2 Batch 560 Loss 2.2250\n",
            "Epoch 2 Batch 580 Loss 2.1632\n",
            "Epoch 2 Batch 600 Loss 2.0959\n",
            "Epoch 2 Batch 620 Loss 2.1196\n",
            "Epoch 2 Batch 640 Loss 1.9401\n",
            "Epoch 2 Batch 660 Loss 1.8244\n",
            "Epoch 2 Batch 680 Loss 1.8889\n",
            "Epoch 2 Batch 700 Loss 1.8449\n",
            "Epoch 2 Batch 720 Loss 1.4859\n",
            "Epoch 2 Batch 740 Loss 2.4054\n",
            "Epoch 2 Batch 760 Loss 1.8482\n",
            "Epoch 2 Batch 780 Loss 1.8938\n",
            "Epoch 2 Batch 800 Loss 1.9489\n",
            "Epoch 2 Batch 820 Loss 2.0343\n",
            "Epoch 2 Batch 840 Loss 2.4121\n",
            "Epoch 2 Batch 860 Loss 2.0607\n",
            "Epoch 2 Batch 880 Loss 1.8821\n",
            "Epoch 2 Batch 900 Loss 2.0110\n",
            "Epoch 2 Batch 920 Loss 2.2539\n",
            "Epoch 2 Batch 940 Loss 1.7277\n",
            "Epoch 2 Batch 960 Loss 1.5389\n",
            "Epoch 2 Batch 980 Loss 2.0261\n",
            "Epoch 2 Batch 1000 Loss 2.0714\n",
            "Epoch 2 Batch 1020 Loss 2.1398\n",
            "Epoch 2 Batch 1040 Loss 1.8093\n",
            "Epoch 2 Batch 1060 Loss 2.0902\n",
            "Epoch 2 Batch 1080 Loss 1.7586\n",
            "Epoch 2 Batch 1100 Loss 1.9556\n",
            "Epoch 2 Batch 1120 Loss 1.8006\n",
            "Epoch 2 Batch 1140 Loss 1.9309\n",
            "Epoch 2 Batch 1160 Loss 1.9608\n",
            "Epoch 2 Batch 1180 Loss 1.5746\n",
            "Epoch 2 Batch 1200 Loss 2.0196\n",
            "Epoch 2 Batch 1220 Loss 1.6637\n",
            "Epoch 2 Batch 1240 Loss 2.0301\n",
            "Epoch 2 Batch 1260 Loss 1.9193\n",
            "Epoch 2 Batch 1280 Loss 1.7034\n",
            "Epoch 2 Batch 1300 Loss 1.8873\n",
            "Epoch 2 Batch 1320 Loss 1.9478\n",
            "Epoch 2 Batch 1340 Loss 1.5869\n",
            "Epoch 2 Batch 1360 Loss 2.2010\n",
            "Epoch 2 Batch 1380 Loss 2.0758\n",
            "Epoch 2 Batch 1400 Loss 1.6142\n",
            "Epoch 2 Batch 1420 Loss 1.8096\n",
            "Epoch 2 Batch 1440 Loss 1.5249\n",
            "Epoch 2 Batch 1460 Loss 1.6690\n",
            "Epoch 2 Batch 1480 Loss 1.9367\n",
            "Epoch 2 Batch 1500 Loss 1.8573\n",
            "Epoch 2 Batch 1520 Loss 1.7419\n",
            "Epoch 2 Batch 1540 Loss 1.7877\n",
            "Epoch 2 Batch 1560 Loss 1.6644\n",
            "Epoch 2 Batch 1580 Loss 1.4350\n",
            "Epoch 2 Batch 1600 Loss 1.7046\n",
            "Epoch 2 Batch 1620 Loss 1.7440\n",
            "Epoch 2 Batch 1640 Loss 1.8310\n",
            "Epoch 2 Batch 1660 Loss 2.0243\n",
            "Epoch 2 Batch 1680 Loss 1.7190\n",
            "Epoch 2 Batch 1700 Loss 1.6208\n",
            "Epoch 2 Batch 1720 Loss 1.9306\n",
            "Epoch 2 Batch 1740 Loss 1.8201\n",
            "Epoch 2 Batch 1760 Loss 1.8754\n",
            "Epoch 2 Batch 1780 Loss 2.1060\n",
            "Epoch 2 Batch 1800 Loss 1.9191\n",
            "Epoch 2 Batch 1820 Loss 1.7401\n",
            "Epoch 2 Batch 1840 Loss 1.6934\n",
            "Epoch 2 Batch 1860 Loss 1.7464\n",
            "Epoch 2 Batch 1880 Loss 1.9736\n",
            "Epoch 2 Batch 1900 Loss 2.1703\n",
            "Epoch 2 Batch 1920 Loss 1.8402\n",
            "Epoch 2 Batch 1940 Loss 2.1689\n",
            "Epoch 2 Batch 1960 Loss 1.7571\n",
            "Epoch 2 Batch 1980 Loss 1.7912\n",
            "Epoch 2 Batch 2000 Loss 1.6093\n",
            "Epoch 2 Batch 2020 Loss 1.8473\n",
            "Epoch 2 Batch 2040 Loss 1.8145\n",
            "Epoch 2 Batch 2060 Loss 2.2634\n",
            "Epoch 2 Batch 2080 Loss 1.6711\n",
            "Epoch 2 Batch 2100 Loss 1.9886\n",
            "Epoch 2 Batch 2120 Loss 1.7272\n",
            "Epoch 2 Batch 2140 Loss 2.0475\n",
            "Epoch 2 Batch 2160 Loss 1.9035\n",
            "Epoch 2 Batch 2180 Loss 1.8938\n",
            "Epoch 2 Batch 2200 Loss 1.6276\n",
            "Epoch 2 Batch 2220 Loss 1.9022\n",
            "Epoch 2 Batch 2240 Loss 1.9644\n",
            "Epoch 2 Batch 2260 Loss 1.9862\n",
            "Epoch 2 Batch 2280 Loss 1.8442\n",
            "Epoch 2 Batch 2300 Loss 2.1238\n",
            "Epoch 2 Batch 2320 Loss 1.6463\n",
            "Epoch 2 Batch 2340 Loss 1.6856\n",
            "Epoch 2 Batch 2360 Loss 2.0798\n",
            "Epoch 2 Batch 2380 Loss 1.7463\n",
            "Epoch 2 Batch 2400 Loss 1.9659\n",
            "Epoch 2 Batch 2420 Loss 1.8256\n",
            "Epoch 2 Batch 2440 Loss 1.9459\n",
            "Epoch 2 Batch 2460 Loss 2.0649\n",
            "Epoch 2 Batch 2480 Loss 1.7000\n",
            "Epoch 2 Batch 2500 Loss 1.7362\n",
            "Epoch 2 Batch 2520 Loss 1.8708\n",
            "Epoch 2 Batch 2540 Loss 1.8001\n",
            "Epoch 2 Batch 2560 Loss 1.8738\n",
            "Epoch 2 Batch 2580 Loss 1.6756\n",
            "Epoch 2 Batch 2600 Loss 2.1462\n",
            "Epoch 2 Batch 2620 Loss 2.1668\n",
            "Epoch 2 Batch 2640 Loss 1.6731\n",
            "Epoch 2 Batch 2660 Loss 1.6853\n",
            "Epoch 2 Batch 2680 Loss 1.6781\n",
            "Epoch 2 Batch 2700 Loss 2.0615\n",
            "Epoch 2 Batch 2720 Loss 1.7296\n",
            "Epoch 2 Batch 2740 Loss 1.4413\n",
            "Epoch 2 Batch 2760 Loss 1.6676\n",
            "Epoch 2 Batch 2780 Loss 1.8235\n",
            "Epoch 2 Batch 2800 Loss 1.6286\n",
            "Epoch 2 Batch 2820 Loss 1.7543\n",
            "Epoch 2 Batch 2840 Loss 1.9978\n",
            "Epoch 2 Batch 2860 Loss 1.9763\n",
            "Epoch 2 Batch 2880 Loss 1.6385\n",
            "Epoch 2 Batch 2900 Loss 1.7636\n",
            "Epoch 2 Batch 2920 Loss 1.6652\n",
            "Epoch 2 Batch 2940 Loss 1.7571\n",
            "Epoch 2 Batch 2960 Loss 1.9190\n",
            "Epoch 2 Batch 2980 Loss 1.9445\n",
            "Epoch 2 Batch 3000 Loss 2.2101\n",
            "Epoch 2 Batch 3020 Loss 1.7103\n",
            "Epoch 2 Batch 3040 Loss 2.1384\n",
            "Epoch 2 Batch 3060 Loss 2.1306\n",
            "Epoch 2 Batch 3080 Loss 1.9343\n",
            "Epoch 2 Batch 3100 Loss 2.0638\n",
            "Epoch 2 Batch 3120 Loss 1.7288\n",
            "Epoch 2 Batch 3140 Loss 1.9516\n",
            "Epoch 2 Batch 3160 Loss 1.8560\n",
            "Epoch 2 Batch 3180 Loss 1.8089\n",
            "Epoch 2 Batch 3200 Loss 1.4947\n",
            "Epoch 2 Batch 3220 Loss 1.3175\n",
            "Epoch 2 Batch 3240 Loss 1.8617\n",
            "Epoch 2 Batch 3260 Loss 2.1169\n",
            "Epoch 2 Batch 3280 Loss 2.0173\n",
            "Epoch 2 Batch 3300 Loss 1.6147\n",
            "Epoch 2 Batch 3320 Loss 1.5532\n",
            "Epoch 2 Batch 3340 Loss 1.7443\n",
            "Epoch 2 Batch 3360 Loss 1.8822\n",
            "Epoch 2 Batch 3380 Loss 1.7439\n",
            "Epoch 2 Batch 3400 Loss 2.1347\n",
            "Epoch 2 Batch 3420 Loss 1.8182\n",
            "Epoch 2 Batch 3440 Loss 1.7249\n",
            "Epoch 2 Batch 3460 Loss 1.5785\n",
            "Epoch 2 Batch 3480 Loss 1.6463\n",
            "Epoch 2 Batch 3500 Loss 1.9928\n",
            "Epoch 2 Batch 3520 Loss 1.7666\n",
            "Epoch 2 Batch 3540 Loss 1.8047\n",
            "Epoch 2 Batch 3560 Loss 2.0874\n",
            "Epoch 2 Batch 3580 Loss 1.9296\n",
            "Epoch 2 Batch 3600 Loss 1.9821\n",
            "Epoch 2 Batch 3620 Loss 1.7830\n",
            "Epoch 2 Batch 3640 Loss 1.9023\n",
            "Epoch 2 Batch 3660 Loss 1.6301\n",
            "Epoch 2 Batch 3680 Loss 1.7095\n",
            "Epoch 2 Batch 3700 Loss 2.1243\n",
            "Epoch 2 Batch 3720 Loss 2.0224\n",
            "Epoch 2 Batch 3740 Loss 1.9436\n",
            "Epoch 2 Batch 3760 Loss 1.8640\n",
            "Epoch 2 Batch 3780 Loss 2.0697\n",
            "Epoch 2 Batch 3800 Loss 1.7416\n",
            "Epoch 2 Batch 3820 Loss 1.5197\n",
            "Epoch 2 Batch 3840 Loss 1.9483\n",
            "Epoch 2 Batch 3860 Loss 1.7865\n",
            "Epoch 2 Batch 3880 Loss 1.9872\n",
            "Epoch 2 Batch 3900 Loss 1.5862\n",
            "Epoch 2 Batch 3920 Loss 1.7186\n",
            "Epoch 2 Batch 3940 Loss 2.1636\n",
            "Epoch 2 Batch 3960 Loss 1.9011\n",
            "Epoch 2 Batch 3980 Loss 2.0026\n",
            "Epoch 2 Batch 4000 Loss 1.6516\n",
            "Epoch 2 Batch 4020 Loss 1.7150\n",
            "Epoch 2 Batch 4040 Loss 1.8707\n",
            "Epoch 2 Loss 1.8940\n",
            "Time taken for 1 epoch 881.9850373268127 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7858\n",
            "Epoch 3 Batch 20 Loss 1.7390\n",
            "Epoch 3 Batch 40 Loss 1.7895\n",
            "Epoch 3 Batch 60 Loss 2.0520\n",
            "Epoch 3 Batch 80 Loss 1.7114\n",
            "Epoch 3 Batch 100 Loss 2.0122\n",
            "Epoch 3 Batch 120 Loss 1.5303\n",
            "Epoch 3 Batch 140 Loss 1.8230\n",
            "Epoch 3 Batch 160 Loss 1.9647\n",
            "Epoch 3 Batch 180 Loss 1.6410\n",
            "Epoch 3 Batch 200 Loss 1.2860\n",
            "Epoch 3 Batch 220 Loss 1.7482\n",
            "Epoch 3 Batch 240 Loss 1.6896\n",
            "Epoch 3 Batch 260 Loss 1.7393\n",
            "Epoch 3 Batch 280 Loss 1.7182\n",
            "Epoch 3 Batch 300 Loss 1.6831\n",
            "Epoch 3 Batch 320 Loss 1.6779\n",
            "Epoch 3 Batch 340 Loss 1.4434\n",
            "Epoch 3 Batch 360 Loss 1.8254\n",
            "Epoch 3 Batch 380 Loss 1.7123\n",
            "Epoch 3 Batch 400 Loss 2.0829\n",
            "Epoch 3 Batch 420 Loss 2.0766\n",
            "Epoch 3 Batch 440 Loss 1.5718\n",
            "Epoch 3 Batch 460 Loss 1.8335\n",
            "Epoch 3 Batch 480 Loss 1.6687\n",
            "Epoch 3 Batch 500 Loss 2.2811\n",
            "Epoch 3 Batch 520 Loss 1.9027\n",
            "Epoch 3 Batch 540 Loss 1.6799\n",
            "Epoch 3 Batch 560 Loss 2.1169\n",
            "Epoch 3 Batch 580 Loss 2.0654\n",
            "Epoch 3 Batch 600 Loss 2.0151\n",
            "Epoch 3 Batch 620 Loss 2.0425\n",
            "Epoch 3 Batch 640 Loss 1.8550\n",
            "Epoch 3 Batch 660 Loss 1.7467\n",
            "Epoch 3 Batch 680 Loss 1.7977\n",
            "Epoch 3 Batch 700 Loss 1.7666\n",
            "Epoch 3 Batch 720 Loss 1.4193\n",
            "Epoch 3 Batch 740 Loss 2.3328\n",
            "Epoch 3 Batch 760 Loss 1.7691\n",
            "Epoch 3 Batch 780 Loss 1.7982\n",
            "Epoch 3 Batch 800 Loss 1.8637\n",
            "Epoch 3 Batch 820 Loss 1.9446\n",
            "Epoch 3 Batch 840 Loss 2.3193\n",
            "Epoch 3 Batch 860 Loss 1.9737\n",
            "Epoch 3 Batch 880 Loss 1.7994\n",
            "Epoch 3 Batch 900 Loss 1.9103\n",
            "Epoch 3 Batch 920 Loss 2.1845\n",
            "Epoch 3 Batch 940 Loss 1.6566\n",
            "Epoch 3 Batch 960 Loss 1.4856\n",
            "Epoch 3 Batch 980 Loss 1.9600\n",
            "Epoch 3 Batch 1000 Loss 2.0020\n",
            "Epoch 3 Batch 1020 Loss 2.0726\n",
            "Epoch 3 Batch 1040 Loss 1.7334\n",
            "Epoch 3 Batch 1060 Loss 2.0045\n",
            "Epoch 3 Batch 1080 Loss 1.6856\n",
            "Epoch 3 Batch 1100 Loss 1.8664\n",
            "Epoch 3 Batch 1120 Loss 1.7319\n",
            "Epoch 3 Batch 1140 Loss 1.8460\n",
            "Epoch 3 Batch 1160 Loss 1.8841\n",
            "Epoch 3 Batch 1180 Loss 1.5037\n",
            "Epoch 3 Batch 1200 Loss 1.9502\n",
            "Epoch 3 Batch 1220 Loss 1.6013\n",
            "Epoch 3 Batch 1240 Loss 1.9482\n",
            "Epoch 3 Batch 1260 Loss 1.8547\n",
            "Epoch 3 Batch 1280 Loss 1.6233\n",
            "Epoch 3 Batch 1300 Loss 1.8024\n",
            "Epoch 3 Batch 1320 Loss 1.8520\n",
            "Epoch 3 Batch 1340 Loss 1.5209\n",
            "Epoch 3 Batch 1360 Loss 2.1281\n",
            "Epoch 3 Batch 1380 Loss 1.9743\n",
            "Epoch 3 Batch 1400 Loss 1.5482\n",
            "Epoch 3 Batch 1420 Loss 1.7334\n",
            "Epoch 3 Batch 1440 Loss 1.4700\n",
            "Epoch 3 Batch 1460 Loss 1.6218\n",
            "Epoch 3 Batch 1480 Loss 1.8664\n",
            "Epoch 3 Batch 1500 Loss 1.7812\n",
            "Epoch 3 Batch 1520 Loss 1.6706\n",
            "Epoch 3 Batch 1540 Loss 1.7113\n",
            "Epoch 3 Batch 1560 Loss 1.5970\n",
            "Epoch 3 Batch 1580 Loss 1.3928\n",
            "Epoch 3 Batch 1600 Loss 1.6464\n",
            "Epoch 3 Batch 1620 Loss 1.6890\n",
            "Epoch 3 Batch 1640 Loss 1.7820\n",
            "Epoch 3 Batch 1660 Loss 1.9318\n",
            "Epoch 3 Batch 1680 Loss 1.6465\n",
            "Epoch 3 Batch 1700 Loss 1.5831\n",
            "Epoch 3 Batch 1720 Loss 1.8884\n",
            "Epoch 3 Batch 1740 Loss 1.7646\n",
            "Epoch 3 Batch 1760 Loss 1.8224\n",
            "Epoch 3 Batch 1780 Loss 2.0497\n",
            "Epoch 3 Batch 1800 Loss 1.8534\n",
            "Epoch 3 Batch 1820 Loss 1.6854\n",
            "Epoch 3 Batch 1840 Loss 1.6273\n",
            "Epoch 3 Batch 1860 Loss 1.6831\n",
            "Epoch 3 Batch 1880 Loss 1.8880\n",
            "Epoch 3 Batch 1900 Loss 2.0966\n",
            "Epoch 3 Batch 1920 Loss 1.7888\n",
            "Epoch 3 Batch 1940 Loss 2.1043\n",
            "Epoch 3 Batch 1960 Loss 1.7172\n",
            "Epoch 3 Batch 1980 Loss 1.7315\n",
            "Epoch 3 Batch 2000 Loss 1.5584\n",
            "Epoch 3 Batch 2020 Loss 1.7875\n",
            "Epoch 3 Batch 2040 Loss 1.7643\n",
            "Epoch 3 Batch 2060 Loss 2.1683\n",
            "Epoch 3 Batch 2080 Loss 1.6158\n",
            "Epoch 3 Batch 2100 Loss 1.9251\n",
            "Epoch 3 Batch 2120 Loss 1.6657\n",
            "Epoch 3 Batch 2140 Loss 1.9866\n",
            "Epoch 3 Batch 2160 Loss 1.8331\n",
            "Epoch 3 Batch 2180 Loss 1.8347\n",
            "Epoch 3 Batch 2200 Loss 1.5729\n",
            "Epoch 3 Batch 2220 Loss 1.8378\n",
            "Epoch 3 Batch 2240 Loss 1.9047\n",
            "Epoch 3 Batch 2260 Loss 1.9154\n",
            "Epoch 3 Batch 2280 Loss 1.7818\n",
            "Epoch 3 Batch 2300 Loss 2.0559\n",
            "Epoch 3 Batch 2320 Loss 1.6031\n",
            "Epoch 3 Batch 2340 Loss 1.6349\n",
            "Epoch 3 Batch 2360 Loss 2.0253\n",
            "Epoch 3 Batch 2380 Loss 1.6908\n",
            "Epoch 3 Batch 2400 Loss 1.8996\n",
            "Epoch 3 Batch 2420 Loss 1.7675\n",
            "Epoch 3 Batch 2440 Loss 1.8565\n",
            "Epoch 3 Batch 2460 Loss 2.0106\n",
            "Epoch 3 Batch 2480 Loss 1.6547\n",
            "Epoch 3 Batch 2500 Loss 1.6665\n",
            "Epoch 3 Batch 2520 Loss 1.8066\n",
            "Epoch 3 Batch 2540 Loss 1.7483\n",
            "Epoch 3 Batch 2560 Loss 1.8183\n",
            "Epoch 3 Batch 2580 Loss 1.6147\n",
            "Epoch 3 Batch 2600 Loss 2.0702\n",
            "Epoch 3 Batch 2620 Loss 2.0985\n",
            "Epoch 3 Batch 2640 Loss 1.6234\n",
            "Epoch 3 Batch 2660 Loss 1.6437\n",
            "Epoch 3 Batch 2680 Loss 1.6152\n",
            "Epoch 3 Batch 2700 Loss 2.0053\n",
            "Epoch 3 Batch 2720 Loss 1.6582\n",
            "Epoch 3 Batch 2740 Loss 1.4011\n",
            "Epoch 3 Batch 2760 Loss 1.6191\n",
            "Epoch 3 Batch 2780 Loss 1.7611\n",
            "Epoch 3 Batch 2800 Loss 1.5783\n",
            "Epoch 3 Batch 2820 Loss 1.6966\n",
            "Epoch 3 Batch 2840 Loss 1.9444\n",
            "Epoch 3 Batch 2860 Loss 1.9190\n",
            "Epoch 3 Batch 2880 Loss 1.5774\n",
            "Epoch 3 Batch 2900 Loss 1.7008\n",
            "Epoch 3 Batch 2920 Loss 1.6065\n",
            "Epoch 3 Batch 2940 Loss 1.6959\n",
            "Epoch 3 Batch 2960 Loss 1.8594\n",
            "Epoch 3 Batch 2980 Loss 1.8976\n",
            "Epoch 3 Batch 3000 Loss 2.1378\n",
            "Epoch 3 Batch 3020 Loss 1.6697\n",
            "Epoch 3 Batch 3040 Loss 2.0772\n",
            "Epoch 3 Batch 3060 Loss 2.0680\n",
            "Epoch 3 Batch 3080 Loss 1.8732\n",
            "Epoch 3 Batch 3100 Loss 1.9877\n",
            "Epoch 3 Batch 3120 Loss 1.6515\n",
            "Epoch 3 Batch 3140 Loss 1.8893\n",
            "Epoch 3 Batch 3160 Loss 1.8012\n",
            "Epoch 3 Batch 3180 Loss 1.7505\n",
            "Epoch 3 Batch 3200 Loss 1.4351\n",
            "Epoch 3 Batch 3220 Loss 1.2954\n",
            "Epoch 3 Batch 3240 Loss 1.8091\n",
            "Epoch 3 Batch 3260 Loss 2.0491\n",
            "Epoch 3 Batch 3280 Loss 1.9653\n",
            "Epoch 3 Batch 3300 Loss 1.5460\n",
            "Epoch 3 Batch 3320 Loss 1.5090\n",
            "Epoch 3 Batch 3340 Loss 1.6965\n",
            "Epoch 3 Batch 3360 Loss 1.8315\n",
            "Epoch 3 Batch 3380 Loss 1.7008\n",
            "Epoch 3 Batch 3400 Loss 2.0648\n",
            "Epoch 3 Batch 3420 Loss 1.7786\n",
            "Epoch 3 Batch 3440 Loss 1.6681\n",
            "Epoch 3 Batch 3460 Loss 1.5261\n",
            "Epoch 3 Batch 3480 Loss 1.6047\n",
            "Epoch 3 Batch 3500 Loss 1.9047\n",
            "Epoch 3 Batch 3520 Loss 1.6999\n",
            "Epoch 3 Batch 3540 Loss 1.7448\n",
            "Epoch 3 Batch 3560 Loss 2.0261\n",
            "Epoch 3 Batch 3580 Loss 1.8854\n",
            "Epoch 3 Batch 3600 Loss 1.9371\n",
            "Epoch 3 Batch 3620 Loss 1.7406\n",
            "Epoch 3 Batch 3640 Loss 1.8487\n",
            "Epoch 3 Batch 3660 Loss 1.5828\n",
            "Epoch 3 Batch 3680 Loss 1.6826\n",
            "Epoch 3 Batch 3700 Loss 2.0576\n",
            "Epoch 3 Batch 3720 Loss 1.9663\n",
            "Epoch 3 Batch 3740 Loss 1.8938\n",
            "Epoch 3 Batch 3760 Loss 1.8363\n",
            "Epoch 3 Batch 3780 Loss 2.0188\n",
            "Epoch 3 Batch 3800 Loss 1.6961\n",
            "Epoch 3 Batch 3820 Loss 1.4706\n",
            "Epoch 3 Batch 3840 Loss 1.8921\n",
            "Epoch 3 Batch 3860 Loss 1.7334\n",
            "Epoch 3 Batch 3880 Loss 1.9222\n",
            "Epoch 3 Batch 3900 Loss 1.5294\n",
            "Epoch 3 Batch 3920 Loss 1.6735\n",
            "Epoch 3 Batch 3940 Loss 2.1073\n",
            "Epoch 3 Batch 3960 Loss 1.8459\n",
            "Epoch 3 Batch 3980 Loss 1.9438\n",
            "Epoch 3 Batch 4000 Loss 1.6124\n",
            "Epoch 3 Batch 4020 Loss 1.6625\n",
            "Epoch 3 Batch 4040 Loss 1.8157\n",
            "Epoch 3 Loss 1.8282\n",
            "Time taken for 1 epoch 879.5591633319855 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UhZG5Yb3G91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_inp = 19\n",
        "max_length_targ = 19\n",
        "\n",
        "def evaluate(sentence):\n",
        "    sentence = clean_text(sentence)\n",
        "\n",
        "    inputs = [vocab[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([vocab['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += rev_word_index[predicted_id] + ' '\n",
        "\n",
        "        if rev_word_index[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT8oxZ-Qo9qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def respond(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP1IRGFRtn3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8cf6d8e8-a601-4002-edc2-725c7b96c620"
      },
      "source": [
        "respond('hello')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hello <end>\n",
            "Predicted translation: i am not going to be a little . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqzvoADPt5oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "67b9d1ee-0231-4a80-8730-97273ce39f5d"
      },
      "source": [
        "respond('what a beautiful day')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> what a beautiful day <end>\n",
            "Predicted translation: i am not going to be a little . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkgwG_0-1aa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c68bab57-0f64-4116-e222-3f2e46bbb5f5"
      },
      "source": [
        "respond('make this quick')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> make this quick <end>\n",
            "Predicted translation: i am not going to be a little . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dZyHP21rRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe409956-5c1c-4d58-b292-28376a2f3ae4"
      },
      "source": [
        "clean_questions[0]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> well , i thought we would start with pronunciation , if that is okay with you . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocNaW48s26BQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "623b40c7-e4ba-4418-8024-a3a037ca335d"
      },
      "source": [
        "respond('well , i thought we would start with pronunciation , if that is okay with you')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> well , i thought we would start with pronunciation , if that is okay with you <end>\n",
            "Predicted translation: i am not going to be a little . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuwCRS0d3FDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}